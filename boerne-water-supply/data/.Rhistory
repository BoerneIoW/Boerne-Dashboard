rm(api.data, dam.data, lake_level, cons.stor, flood.stor, lake_stor, lake_data, district_data)
} #end of district
summary(all_district_data)  #a few NA's in storage_AF and Elev_Ft - Addicks and Barker, as expected.
new.data.usace <- all_district_data
########################################################################################################################################################################################################################
#
#     ADD OLD AND NEW DATA TOGETHER
#
########################################################################################################################################################################################################################
#pull out unique reservoirs
unique.nid <- unique(new.data.usace$NIDID); unique.nid
#new data
nx <- new.data.usace
dateFormat = nx$date[1]
if(substr(dateFormat,5,5) == "-") {dateFormatFinal = "%Y-%m-%d"}
if(substr(dateFormat,5,5) != "-") {dateFormatFinal = "%m/%d/%Y"}
dateFormatFinal
nx$date <- as.Date(as.character(nx$date), dateFormatFinal)
nx$Year <- year(nx$date)
nx$day_month <- substr(nx$date, 6, 10)
#set julian values
for(i in 1:nrow(nx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nx$Year[i]) == TRUE) {nx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nx$day_month[i]]}
if(leap_year(nx$Year[i]) == FALSE) {nx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nx$day_month[i]]}
print(paste(round(i/nrow(nx)*100,2),"% complete"))
}
#clean data
nx <- nx %>% mutate(elev_Ft = ifelse(elev_Ft <= 0, NA, elev_Ft), storage_AF = ifelse(storage_AF <=0, NA, storage_AF), fstorage_AF = ifelse(fstorage_AF <=0, NA, fstorage_AF))
maxCap <- nx %>% group_by(NIDID) %>% summarize(maxCap = 1.2*quantile(elev_Ft, 0.90, na.rm=TRUE), maxStor = 1.2*quantile(storage_AF, 0.90, na.rm=TRUE), maxfStor = 1.2*quantile(fstorage_AF, 0.90, na.rm=TRUE), .groups="drop");
nx <- nx %>% left_join(maxCap, by="NIDID") %>% mutate(elev_Ft = ifelse(elev_Ft > maxCap, NA, elev_Ft), storage_AF = ifelse(storage_AF > maxStor, NA, storage_AF), fstorage_AF = ifelse(fstorage_AF > maxfStor, NA, fstorage_AF)) %>%
select(-maxCap, -maxStor, -maxfStor)
#include month abbreviations
nx2 <- nx %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month])
#old data
fx <- old.data.usace
dateFormat = fx$date[1]
if(substr(dateFormat,5,5) == "-") {dateFormatFinal = "%Y-%m-%d"}
if(substr(dateFormat,5,5) != "-") {dateFormatFinal = "%m/%d/%Y"}
fx$date <- as.Date(as.character(fx$date), dateFormatFinal)
fx$Year <- year(fx$date)
fx$day_month <- substr(fx$date, 6, 10)
# #set julian values - SHOULDN't BE NEEDED GOING FORWARD.
# for(i in 1:nrow(fx)) { #computationally slow. There's almost certainly a faster way. But it works.
#
#   if(leap_year(fx$Year[i]) == TRUE) {fx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == fx$day_month[i]]}
#   if(leap_year(fx$Year[i]) == FALSE) {fx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == fx$day_month[i]]}
#
#   print(paste(round(i/nrow(fx)*100,2),"% complete"))
# }
#include month abbreviations
fx2 <- fx %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month])
#what is the most recent date?
old.last.date <- fx2 %>% group_by(NIDID) %>% filter(date == max(date, na.rm=TRUE)) %>% select(NIDID, date) %>% distinct() %>% rename(lastDate = date)
#remove anything new after that date
nx2 <- nx2 %>% left_join(old.last.date, by="NIDID") %>% filter(date > lastDate)
fx.2020 <- fx2 %>% filter(Year>=2020) %>% select(NIDID, day_month, OT_Ft, OT_AF) %>% distinct(); #2020 has complete data
nx2 <- merge(nx2, fx.2020, by.x=c("NIDID","day_month"), by.y=c("NIDID","day_month"), all.x=TRUE)
nx2 <- nx2 %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2))
nx2 <- nx2 %>% select(NIDID, name, date, Year, day_month, julian, elev_Ft, storage_AF, OT_Ft, OT_AF, percentStorage, monthAbb, month); #colnames(nx2) <- colnames(fx2)
#combine
fx <- rbind(fx2, nx2)
#make sure no duplicates
fx <- fx %>% distinct()
#arrange by NIDID and date
fx <- fx %>% arrange(NIDID, date)
#SCOTT KERR HAS HAD A NEW SEDIMENT SURVEY
scott.ot = 36639
fx <- fx %>% mutate(OT_AF = ifelse(NIDID == "NC00300" & Year >=2017, 36639, OT_AF))
fx <- fx %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2)) %>% mutate(storage_AF = ifelse(percentStorage > 300, NA, storage_AF), percentStorage = ifelse(percentStorage > 300, NA, percentStorage))
summary(fx)
tx.dams <- fx
tx.dams <- tx.dams %>% mutate(jurisdiction = "USACE")
write.csv(tx.dams, paste0(swd_data, "reservoirs/all_usace_dams.csv"), row.names=FALSE)
# filter out reservoir(s) of interest
canyon.lake <- tx.dams %>% filter(name == "Canyon Lake")
#usace changed reporting units so multiply by 1000 for those dates that are not in the same units as previous observations
canyon.lake <- canyon.lake %>% mutate(storage_AF = ifelse(storage_AF < 1000, storage_AF*1000, storage_AF))
#recalculate storage
canyon.lake <- canyon.lake %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2))
write.csv(canyon.lake, paste0(swd_data, "reservoirs/all_reservoir_data.csv"), row.names=FALSE)
########################################################################################################################################################################################################################
#
#          UPDATE RESERVOIR STATUS AND STATS
#
########################################################################################################################################################################################################################
#fx <- read.csv(paste0(swd_data, "reservoirs/all_canyon_lake"), header = TRUE) #for picking up part-way
fx <- canyon.lake %>% filter(is.na(OT_Ft) == FALSE) #drop sites without operational target
unique.sites <- unique(fx$NIDID)
#set up data frame for stats and include year
stats <- as.data.frame(matrix(nrow=0,ncol=13));        colnames(stats) <- c("nidid", "julian", "min", "flow10", "flow25", "flow50", "flow75", "flow90", "max", "Nobs","startYr","endYr","date");
year.flow  <- as.data.frame(matrix(nrow=0, ncol=10));   colnames(year.flow) <- c("nidid", "name", "date", "year", "julian", "elev_ft","storage_af", "target_ft", "target_af", "percent_storage")
#for (i in 1:length(unique.sites)){  ##### loop not needed because it is only one site
#for (i in 17:length(unique.sites)){  #test
zt <- fx %>% filter(NIDID == unique.sites) %>% filter(Year >= year(start.date))
#summarize annual
zt.stats <- fx %>% group_by(NIDID, julian) %>% summarize(Nobs = n(), min=round(min(percentStorage, na.rm=TRUE),4), flow10 = round(quantile(percentStorage, 0.10, na.rm=TRUE),4), flow25 = round(quantile(percentStorage, 0.25, na.rm=TRUE),4),
flow50 = round(quantile(percentStorage, 0.5, na.rm=TRUE),4), flow75 = round(quantile(percentStorage, 0.75, na.rm=TRUE),4), flow90 = round(quantile(percentStorage, 0.90, na.rm=TRUE),4),
max = round(max(percentStorage, na.rm=TRUE),4), .groups="drop")
zt.stats <- zt.stats %>% mutate(NIDID = as.character(NIDID), startYr = min(fx$Year), endYr = max(fx$Year))
if(dim(zt.stats)[1] == 366) {zt.stats$date = julian.ref$day_month_leap}
if(dim(zt.stats)[1] == 365) {zt.stats$date = julian.ref$day_month[c(1:365)]}
#fill dataframe
stats <- rbind(stats, zt.stats)
zt <- fx %>% filter(Year>=2017) #%>% select(NIDID, name, date, year, julian, elev_Ft, storage_AF, OT_Ft, OT_AF, percentStorage, jurisdiction);
colnames(zt) <- c("NIDID", "name", "date", "year", "day_month", "julian", "elev_ft","storage_af", "target_ft", "target_af", "percent_storage", "month", "monthAbb", "jurisdiction")
year.flow <- rbind(year.flow, zt)
#    print(i)
#  }
bk.up <- stats
summary(stats)
summary(year.flow)
#fix date stuff
stats2 <- stats
stats2$endYr <- as.character(stats$endYr)
stats2$startYr <- as.character(stats$startYr)
stats2 <- stats2 %>% mutate(date2 = as.Date(paste0(end.year, "-",date)))
stats2 <- stats2 %>% mutate(month = substr(date,0,2))
stats2 <- stats2 %>% mutate(month = ifelse(month == "01", "Jan", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "07", "Jul", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "02", "Feb", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "08", "Aug", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "03", "Mar", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "09", "Sep", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "04", "Apr", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "10", "Oct", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "05", "May", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "11", "Nov", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "06", "Jun", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "12", "Dec", month))
#Now attach most recent value to stream stats for the map
recent.flow <- year.flow %>% group_by(NIDID) %>% filter(is.na(storage_af) == FALSE) %>% filter(date == max(date)); #do we want to do most recent date or most recent date with data?
current.stat <- merge(recent.flow, stats2, by.x=c("NIDID","julian"), by.y=c("NIDID", "julian"), all.x=TRUE) #%>% rename(date = date.x)
#clean
current.stat <- current.stat %>% select(-date.x, -year, -date.y, -elev_ft, -storage_af, -target_af, -target_ft, -month.x, -jurisdiction, -month.y)
current.stat <- current.stat %>% rename(date = day_month, month = monthAbb)
#if else for this year and last years flow
current.stat <- current.stat %>% mutate(status = ifelse(percent_storage <= flow10, "Extremely Dry", ifelse(percent_storage > flow10 & percent_storage <= flow25, "Very Dry", ifelse(percent_storage >= flow25 & percent_storage < flow50, "Moderately Dry",
ifelse(percent_storage >= flow50 & percent_storage < flow75, "Moderately Wet", ifelse(percent_storage >= flow75 & percent_storage < flow90, "Very Wet", ifelse(percent_storage >= flow90, "Extremely Wet", "Unknown")))))))
current.stat$status <- ifelse(is.na(current.stat$status), "unknown", current.stat$status)
table(current.stat$status)
#merge to sites geojson
project.df <- read_sf(paste0(swd_data, "reservoirs/all_canyon_lake_site.geojson"))
res.loc <- project.df %>% select(NIDID, Name, Jurisdiction, geometry) #why are there 4 of everything?
res.loc <- res.loc %>% slice(1)
canyon.lake.site.stats <- merge(res.loc, current.stat[,c("NIDID","status","percent_storage","julian","flow50")], by.x="NIDID", by.y="NIDID") #why are there 4 of everything?
#mapview::mapview(canyon.lake.site.stats)
geojson_write(canyon.lake.site.stats, file=paste0(swd_data, "reservoirs/all_canyon_lake_site.geojson"))
#rename nidid to site so can use same code as streamflow - used to make charts
current.year <- year.flow %>% filter(year == year(max(date)));     last.year <- year.flow %>% filter(year == (year(max(date))-1));
stats.flow <- merge(stats, current.year, by.x=c("NIDID","julian"), by.y=c("NIDID","julian"), all.x=TRUE) %>% rename(site = NIDID, flow = percent_storage)
stats.past <- merge(stats, last.year, by.x=c("NIDID", "julian"), by.y=c("NIDID", "julian"), all.x=TRUE) %>% rename(site = NIDID, flow = percent_storage) %>% as.data.frame()
#clean and bind
stats.flow <- stats.flow %>% select(-date.x, -name, -year, -elev_ft, -storage_af, -target_af, -target_ft, -month, -jurisdiction)
stats.flow <- stats.flow %>% rename(date = day_month, date2 = date.y, month = monthAbb)
stats.past <- stats.past %>% select(-date.x, -name, -year, -elev_ft, -storage_af, -target_af, -target_ft, -month, -jurisdiction)
stats.past <- stats.past %>% rename(date = day_month, date2 = date.y, month = monthAbb)
stats.flow <- rbind(stats.past, stats.flow)
#get status
stats.flow <- stats.flow %>% mutate(status = ifelse(flow <= flow10, "Extremely Dry", ifelse(flow > flow10 & flow <= flow25, "Very Dry", ifelse(flow >= flow25 & flow < flow50, "Moderately Dry",
ifelse(flow >= flow50 & flow < flow75, "Moderately Wet", ifelse(flow >= flow75 & flow < flow90, "Very Wet", ifelse(flow >= flow90, "Extremely Wet", "Unknown")))))))
stats.flow <- stats.flow %>% mutate(colorStatus = ifelse(status=="Extremely Dry", "darkred", ifelse(status=="Very Dry", "red", ifelse(status=="Moderately Dry", "orange", ifelse(status=="Moderately Wet", "cornflowerblue",
ifelse(status=="Very Wet", "blue", ifelse(status=="Extremely Wet", "navy", "gray")))))))
#save out
write.csv(stats.flow, paste0(swd_data, "reservoirs/all_reservoir_stats.csv"), row.names=FALSE)
################################################################################################################################################################
# remove all except for global environment
rm(list= ls()[!(ls() %in% c('julian.ref','update.date', 'current.month', 'current.year', 'end.date', 'end.year',
'mymonths', 'source_path', 'start.date', 'state_fips', 'stateAbb', 'stateFips', 'swd_data', 'today',
'%notin%', 'ma'))])
boerne.sites <- read.csv(paste0(swd_data, "gw/well_metadata.csv"))
old.data <- read.csv(paste0(swd_data, "gw/historic_gw_depth.csv")) %>% mutate(date = as.Date(date, format="%Y-%m-%d"))
#double-check that each column is the desired type (numeric, character, etc.) and make necessary changes
str(old.data) # site = chr; date = Date, format; julian = int; depth_ft = num
old.data$site <- as.character(old.data$site)
str(old.data)
######################################################################################################################################################################
#
# PULL IN GW LEVEL DATA DYNAMICALLY
#
#####################################################################################################################################################################
startDate <- max(old.data$date) + 1
endDate <- as.Date(today)
#authenticate account
gs4_auth()
# number of sheets to be imported
sheet.number <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
21, 22, 23, 24, 25, 26, 27, 28, 29, 30,
31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
41, 42) #this is the part that changes
# create empty storage dfs
all.well.metadata <- matrix(nrow = 0, ncol = 26) %>% as.data.frame()
colnames(all.well.metadata) <- c("...1", "...2", "...3", "...4", "...5", "...6", "...7", "...8", "...9", "...10",
"...11", "...12", "...13", "...14", "...15", "...16", "...17", "...18", "...19", "...20",
"...21", "...22", "...23", "...24", "Long_Va", "Lat_Va")
all.well.data <- matrix(nrow = 0, ncol = 4) %>% as.data.frame()
colnames(all.well.data) <- c("State_Numer", "...1", "...2", "...3")
# loop through sites and pull data
for(i in 1:length(sheet.number)) {
gw.i.metadata <- read_sheet("https://docs.google.com/spreadsheets/d/1QoaOhrpz6vrSMBc0yc5-i7nhwj2lmsBHZFYOBJc0KVU/edit#gid=1522547605", sheet = sheet.number[i], range = "A2:X3", col_names = FALSE)
gw.i.data <- read_sheet("https://docs.google.com/spreadsheets/d/1QoaOhrpz6vrSMBc0yc5-i7nhwj2lmsBHZFYOBJc0KVU/edit#gid=1522547605", sheet = sheet.number[i], range = "A6:C", col_names = FALSE)
gw.i.metadata$Long_Va <- gw.i.metadata [2,2]
gw.i.metadata$Lat_Va <- gw.i.metadata [2,3]
gw.i.metadata <- gw.i.metadata[-c(2), ]
gw.i.metadata$Long_Va <- unlist(gw.i.metadata$Long_Va)
gw.i.metadata$Lat_Va <- unlist(gw.i.metadata$Lat_Va)
gw.i.data$State_Number <- gw.i.metadata [1,15]
# Now bind it up to save out
all.well.metadata <- rbind(all.well.metadata, gw.i.metadata)
all.well.data <- rbind(all.well.data, gw.i.data)
# Keep an eye on the progress:
print(paste0("Completed pull for ", sheet.number[i], ". ", round(i*100/length(sheet.number), 2), "% complete."))
}
#clean up data
#rename columns
boerne_all_gw_levels <- rename(all.well.data, site = State_Number, date = "...1", depth_ft = "...2", elevation_at_waterlevel = "...3")
colnames(boerne_all_gw_levels)
#double-check that each column is the desired type (numeric, character, etc.) and make necessary changes
str(boerne_all_gw_levels)
boerne_all_gw_levels$site <- unlist(boerne_all_gw_levels$site)
boerne_all_gw_levels$site <- as.character(boerne_all_gw_levels$site)
boerne_all_gw_levels$date <- format(as.Date(boerne_all_gw_levels$date), "%Y-%m-%d")
boerne_all_gw_levels <- as.data.frame(boerne_all_gw_levels)
#remove na's
boerne_all_gw_levels <- na.omit(boerne_all_gw_levels)
boerne_all_gw_levels$depth_ft <- unlist(boerne_all_gw_levels$depth_ft)
boerne_all_gw_levels <- na.omit(boerne_all_gw_levels)
#add julian indexing
nx <- boerne_all_gw_levels %>% mutate(year = year(date), day_month = substr(date, 6, 10))
for(i in 1:nrow(nx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nx$year[i]) == TRUE) {nx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nx$day_month[i]]}
if(leap_year(nx$year[i]) == FALSE) {nx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nx$day_month[i]]}
print(paste(round(i/nrow(nx)*100,2),"% complete"))
}
boerne_all_gw_levels <- nx
boerne_all_gw_levels$agency <- "CCGCD" #include agency that collects data
#remove missing data
boerne_all_gw_levels <- na.omit(boerne_all_gw_levels)
# filter data starting in 2022
new_boerne_gw_levels <- boerne_all_gw_levels %>% filter(year >= 2022)
new_boerne_gw_depth <- select(new_boerne_gw_levels, c(1, 2, 4, 7))
check.last.date <- new_boerne_gw_depth %>% filter(date == max(date)) %>% dplyr::select(date)
table(check.last.date$date)
#combine old and new
all_boerne_gw_depth <- rbind(old.data, new_boerne_gw_depth) %>% arrange(site, date)
#double-check that each column is the desired type (numeric, character, etc.) and make necessary changes
str(all_boerne_gw_depth) # site = chr; date = Date, format; julian = int; depth_ft = num
all_boerne_gw_depth$julian <- as.integer(all_boerne_gw_depth$julian)
write.csv(all_boerne_gw_depth, paste0(swd_data, "gw/all_gw_depth.csv"), row.names=FALSE)
#####################################################################################################################################################################
#given sparse daily data, aggregate to monthly and get average
year.flow <- all_boerne_gw_depth
year.flow$date2 <- floor_date(year.flow$date, "month")
year.flow2 <- year.flow %>% group_by(site, date2) %>% summarize(mean_depth_ft = mean(depth_ft))
year.flow2 <- year.flow2 %>% mutate(month = substr(date2, 6, 7))
year.flow <- year.flow2
year.flow$date2 <- as.character(year.flow$date2)
year.flow <- rename(year.flow, date = date2)
#re-add julian indexing
nx <- year.flow %>% mutate(year = year(date), day_month = substr(date, 6, 10))
for(i in 1:nrow(nx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nx$year[i]) == TRUE) {nx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nx$day_month[i]]}
if(leap_year(nx$year[i]) == FALSE) {nx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nx$day_month[i]]}
print(paste(round(i/nrow(nx)*100,2),"% complete"))
}
year.flow <- nx
#fix variable type to save out monthly averages
year.flow$month <- as.numeric(year.flow$month)
all_boerne_monthly_avg <- select(year.flow, c(1, 2, 3, 4, 5, 7))
write.csv(all_boerne_monthly_avg, paste0(swd_data, "gw/all_monthly_avg.csv"), row.names=FALSE)
#stats calculations of daily data:
year.flow <- all_boerne_gw_depth
#account for any duplicates
#  year.flow <- year.flow %>% group_by(site, date, julian) %>% summarize(depth_ft = median(depth_ft, na.rm=TRUE), .groups="drop")
stats <- as.data.frame(matrix(nrow=0,ncol=13));        colnames(stats) <- c("site", "julian", "min", "flow10", "flow25", "flow50", "flow75", "flow90", "max", "Nobs","startYr","endYr","date");
#Calculate stats for all data... this takes a long while to do in tidyverse... like to see it making progress in loop
unique.sites <- unique(year.flow$site)
for (i in 1:length(unique.sites)){
zt <- year.flow %>% filter(site==unique.sites[i]) %>% mutate(year = year(date)) %>% filter(is.na(depth_ft)==FALSE)
zt.stats <- zt %>% group_by(site, julian) %>% summarize(Nobs = n(), min=round(min(depth_ft, na.rm=TRUE),4), flow10 = round(quantile(depth_ft, 0.10, na.rm=TRUE),4), flow25 = round(quantile(depth_ft, 0.25, na.rm=TRUE),4),
flow50 = round(quantile(depth_ft, 0.5, na.rm=TRUE),4), flow75 = round(quantile(depth_ft, 0.75, na.rm=TRUE),4), flow90 = round(quantile(depth_ft, 0.90, na.rm=TRUE),4),
max = round(max(depth_ft, na.rm=TRUE),4), .groups="drop")
zt.stats <- zt.stats %>%  mutate(startYr = min(zt$year), endYr = max(zt$year)) %>% dplyr::select(site, julian, min, flow10, flow25, flow50, flow75, flow90, max, Nobs, startYr, endYr)
zt.stats$date2 <- as.Date(zt.stats$julian, origin=paste0(current.year,"-01-01"))
zt.stats$date <- format(zt.stats$date2, format="%b-%d")
#if(dim(zt.stats)[1] == 366) {zt.stats$date = julian$month.day366}
#if(dim(zt.stats)[1] < 366) {
#    zt.stats <- merge(zt.stats, julian[,c("julian", "month.day365")], by.x="julian", by.y="julian", all.x=TRUE)
#    zt.stats <- zt.stats %>% rename(date = month.day365)
#} #assumes 365 days... could be wrong
stats <- rbind(stats, zt.stats)
print(paste(i, "is ", round(i/length(unique.sites)*100,2), "percent done"))
}
bk.up <- stats;
is.na(stats) <- sapply(stats, is.infinite)
summary(stats)
#stats <- stats %>% mutate(date2 = as.Date(paste0(current.year,"-",date), format="%Y-%b-%d")) %>% as.data.frame()
head(stats) %>% as.data.frame()
#remove sites that have not had new data in last year
if(month(today)>1){
remove.site <- stats %>% filter(endYr < (current.year-1))  %>% select(site) %>% distinct()
}
######################################################################################################################################################################
#
# CREATE FILES FOR WEBSITE
#
#####################################################################################################################################################################
#Now attach most recent value to stream stats
recent.flow <- year.flow %>% group_by(site) %>% filter(is.na(depth_ft) == FALSE) %>% filter(date == max(date))
#skip the following line if in first month
if(month(today)>1){
recent.flow <- recent.flow %>% filter(julian <= as.POSIXlt(today(), format = "%Y-%m-%d")$yday) %>% filter(site %notin% remove.site$site) #%>% rename(flow = depth_below_surface_ft)
}
current.stat <- merge(recent.flow[,c("site", "julian", "depth_ft")], stats, by.x=c("site","julian"), by.y=c("site","julian"), all.x=TRUE)
#if else for this year and last years flow... I think flip this for gw
current.stat <- current.stat %>% mutate(status = ifelse(depth_ft <= flow10, "Extremely Wet", ifelse(depth_ft > flow10 & depth_ft <= flow25, "Very Wet", ifelse(depth_ft >= flow25 & depth_ft < flow50, "Moderately Wet",
ifelse(depth_ft >= flow50 & depth_ft < flow75, "Moderately Dry", ifelse(depth_ft >= flow75 & depth_ft < flow90, "Very Dry", ifelse(depth_ft >= flow90, "Extremely Dry", "Unknown")))))))
current.stat$status <- ifelse(is.na(current.stat$status), "unknown", current.stat$status)
table(current.stat$status)
#set those that are not collecting data to unknown
max.julian <- current.stat %>% filter(endYr == current.year) %>% summarize(maxJ = max(julian, na.rm=TRUE))
current.stat <- current.stat %>% mutate(status = ifelse(endYr < current.year & julian < 300, "unknown", ifelse(endYr < (current.year-1), "unknown",
ifelse(endYr==current.year & julian < (max.julian$maxJ-60), "unknown", status))))
table(current.stat$status, useNA="ifany")
#merge to sites geojson
boerne.sites  <- boerne.sites %>% rename(site = state_id)
boerne.sites2 <- merge(boerne.sites, current.stat[,c("site","status","depth_ft","julian","date","flow50")], by.x="site", by.y="site") %>% distinct()
#convert to sf
boerne.sites2 <- st_as_sf(boerne.sites2, coords = c("dec_long_va", "dec_lat_va"), crs = 4326);
boerne.sites2 <- merge(boerne.sites2 %>% dplyr::select(-date), recent.flow[,c("site","date")], by.x="site", by.y="site", all.x=TRUE)
#Save out
boerne.sites2 <- boerne.sites2 %>% dplyr::select(agency, site, location, elevation, total_depth, aquifer, status, depth_ft, julian, flow50, date, geometry)
boerne.sites2 <- rename(boerne.sites2, AgencyCd = agency, SiteName = location, WellDepth = total_depth, LocalAquiferName = aquifer)
geojson_write(boerne.sites2, file=paste0(swd_data, "gw/all_gw_sites.geojson"))
#mapview::mapview(boerne.sites2)
#plot for fun
boerne.sites2 <- boerne.sites2 %>% mutate(colorStatus = ifelse(status=="Extremely Dry", "darkred",
ifelse(status=="Very Dry", "red",
ifelse(status=="Moderately Dry", "orange",
ifelse(status=="Moderately Wet", "cornflowerblue",
ifelse(status=="Very Wet", "blue",
ifelse(status=="Extremely Wet", "navy", "gray")))))))
leaflet() %>%  addProviderTiles("Stamen.TonerLite") %>%
addCircleMarkers(data = boerne.sites2, radius=4, fillOpacity= 0.8, fillColor = boerne.sites2$colorStatus, color="black", weight=0)
#Now clip time series data to past two years and assign a depth based on stats
year.flow2 <- year.flow %>% filter(date >= as.Date(paste0((current.year-2),"-01-01"), "%Y-%m-%d"))#limits data to be only for past two years
stats2 <- merge(year.flow2[,c("site", "julian", "date", "depth_ft")], stats %>% dplyr::select(-date), by.x=c("site","julian"), by.y=c("site", "julian"), all.x=TRUE) %>% arrange(site, date)
stats2 <- stats2 %>% mutate(status = ifelse(depth_ft <= flow10, "Extremely Wet", ifelse(depth_ft > flow10 & depth_ft <= flow25, "Very Wet", ifelse(depth_ft >= flow25 & depth_ft < flow50, "Moderately Wet",
ifelse(depth_ft >= flow50 & depth_ft < flow75, "Moderately Dry", ifelse(depth_ft >= flow75 & depth_ft < flow90, "Very Dry", ifelse(depth_ft >= flow90, "Extremely Dry", "Unknown")))))))
stats2$status <- ifelse(is.na(stats2$status), "unknown", stats2$status)
table(stats2$status, useNA="ifany")
stats2 <- stats2 %>% mutate(colorStatus = ifelse(status=="Extremely Dry", "darkred", ifelse(status=="Very Dry", "red", ifelse(status=="Moderately Dry", "orange", ifelse(status=="Moderately Wet", "cornflowerblue",
ifelse(status=="Very Wet", "blue", ifelse(status=="Extremely Wet", "navy", "gray")))))))
stats2 <- stats2 %>% dplyr::select(site, julian, date, depth_ft, status, colorStatus) %>% filter(site %in% boerne.sites$site)
write.csv(stats2, paste0(swd_data, "gw/all_gw_status.csv"), row.names=FALSE)
#set up month names and save out stats file
my.month.name <- Vectorize(function(n) c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct","Nov", "Dec")[n])
recent.flow <- year.flow %>% group_by(site) %>% filter(date >= max(as.Date(paste0(current.year, "-01-01"), '%Y-%m-%d')))
stats.merge <- stats %>% mutate(date3 = date2, date2 = date) %>% dplyr::select(-date) %>% filter(site %in% boerne.sites$site)
current.stat2 <- merge(recent.flow, stats.merge, by.x=c("site","julian"), by.y=c("site","julian"), all.y=TRUE)%>% filter(site %in% boerne.sites2$site)
current.stat2 <- current.stat2 %>% mutate(month = my.month.name(as.numeric(substr(date,6,7)))) %>% mutate(date = date2, date2 = date3) %>% dplyr::select(-date3);  #okay to have NA for date because want chart to end there
write.csv(current.stat2, paste0(swd_data, "gw/all_gw_stats.csv"), row.names=FALSE)
#let's do annual trends
gw.annual <- year.flow %>% mutate(year = year(date)) %>% group_by(site, year) %>% summarize(medianDepth = median(depth_ft, na.rm=TRUE), nobsv = n(), .groups="drop") %>%
filter(site %in% boerne.sites2$site)
write.csv(gw.annual, paste0(swd_data, "gw/all_gw_annual.csv"), row.names=FALSE)
################################################################################################################################################################
# remove all except for global environment
rm(list= ls()[!(ls() %in% c('julian.ref','update.date', 'current.month', 'current.year', 'end.date', 'end.year',
'mymonths', 'source_path', 'start.date', 'state_fips', 'stateAbb', 'stateFips', 'swd_data', 'today',
'%notin%', 'ma'))])
utilities <- read_sf(paste0(swd_data, "utility.geojson"));
pwsid.list <- unique(utilities$pwsid) #Boerne is the utility of interest
mymonths <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"); #used below to convert numbers to abbrev
#mapview::mapview(utilities)
#read in old data
old_total_demand <- read.csv(paste0(swd_data, "demand/historic_total_demand.csv"))
old_demand_by_source <- read.csv(paste0(swd_data, "demand/historic_demand_by_source.csv"))
old_reclaimed <- read.csv(paste0(swd_data, "demand/historic_reclaimed_water.csv"))
old_pop <- read.csv(paste0(swd_data, "demand/historic_pop.csv"))
#calculate moving average function
ma <- function(x,n=7){stats::filter(x,rep(1/n,n), sides=1)}
######################################################################################################################################################################
#
# Read in new water demand data
#
#####################################################################################################################################################################
gs4_auth()
demand_data <- read_sheet("https://docs.google.com/spreadsheets/d/1BKb9Q6UFEBNsGrLZhjdq2kKX5t1GqPFCWF553afUKUg/edit#gid=2030520898", sheet = 1, range = "A229:H", col_names = FALSE,col_types = "Dnnnnnnn")
demand_by_source <- demand_data[, c("...1", "...2", "...3", "...6", "...7", "...8")]
#rename columns
demand_by_mgd <- rename(demand_by_source, date = "...1", groundwater = "...2", boerne_lake = "...3", GBRA = "...6", reclaimed = "...7", total = "...8")
#replace na's with 0s
demand_by_mgd <- as.data.frame(demand_by_mgd)
demand_by_mgd[is.na(demand_by_mgd)] <- 0
demand_by_mgd <- as.data.frame(demand_by_mgd)
#change units to MGD
demand_by_mgd$groundwater <- demand_by_mgd$groundwater/1000; demand_by_mgd$boerne_lake <- demand_by_mgd$boerne_lake/1000; demand_by_mgd$GBRA <- demand_by_mgd$GBRA/1000; demand_by_mgd$reclaimed <- demand_by_mgd$reclaimed/1000; demand_by_mgd$total <- demand_by_mgd$total/1000;
#include PWSId
demand_by_mgd$pwsid <- utilities$pwsid
#add julian indexing
#nx <- demand_by_mgd %>% mutate(year = year(date), month = month(date), day = day(date))
nx <- demand_by_mgd %>% mutate(year = year(date), day_month = substr(date, 6, 10))
for(i in 1:nrow(nx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nx$year[i]) == TRUE) {nx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nx$day_month[i]]}
if(leap_year(nx$year[i]) == FALSE) {nx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nx$day_month[i]]}
print(paste(round(i/nrow(nx)*100,2),"% complete"))
}
demand_by_mgd <- nx
#split date by month and day
demand_by_mgd = demand_by_mgd %>%
mutate(date = ymd(date)) %>%
mutate_at(vars(date), funs(year, month, day))
demand_by_mgd$day <- as.numeric(demand_by_mgd$day)
str(demand_by_mgd)
new_demand_by_mgd <- demand_by_mgd %>% filter(year >= 2022 & date < today)
new_demand_by_mgd$date <- format(as.Date(new_demand_by_mgd$date), "%Y-%m-%d") # make sure the date format is the same for old and new before binding
#remove days that don't have data (utilities director includes 0's for days he hasn't input data yet)
new_demand_by_mgd <- filter(new_demand_by_mgd, groundwater > 0, boerne_lake > 0, GBRA > 0, reclaimed > 0)
#merge old and new data
all_demand_by_mgd <- rbind(old_demand_by_source, new_demand_by_mgd)
check.last.date <- all_demand_by_mgd %>% filter(date == max(date)) %>% dplyr::select(date, month)
table(check.last.date$date)
#write.csv
write.csv(demand_by_mgd, paste0(swd_data, "demand/all_demand_by_source.csv"), row.names=FALSE)
#include month abbreviations
demand2 <- all_demand_by_mgd %>% group_by(pwsid) %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
#calculate mean demand
demand2 <- all_demand_by_mgd %>% mutate(date = as.Date(substr(date,1,10),format='%Y-%m-%d'))
demand3 <- demand2 %>% group_by(pwsid) %>% arrange(date) %>% mutate(timeDays = as.numeric(date - lag(date)))
demand4 <- demand3 %>% group_by(pwsid) %>% mutate(mean_demand = ifelse(timeDays <= 3, round(as.numeric(ma(total)),2), total),
julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
demand5 <- demand4 %>% mutate(total = round(total,2), mean_demand = ifelse(is.na(mean_demand)==TRUE, total, mean_demand))
#calculate monthly peak
demand6 <- demand5 %>% group_by(pwsid, month, year) %>% mutate(peak_demand = round(quantile(total, 0.98),1)); #took the 98% to omit outliers
#provide julian date
demand7 <- demand6 %>% mutate(date2 = date, date = paste0(monthAbb,"-",day(date2))) %>% select(-timeDays)
#clean up
demand7 <- rename(demand7, demand_mgd = "total")
demand7 <- demand7[, c("pwsid", "date","demand_mgd", "mean_demand", "julian", "month", "monthAbb", "year", "peak_demand", "date2")]
#write.csv
write.csv(demand7, paste0(swd_data, "demand/all_total_demand.csv"), row.names=FALSE)
#create comulative demand
demand.data <- demand7 %>% filter(date2>start.date)
foo.count <- demand.data %>% group_by(pwsid, year) %>% count() %>% filter(year < current.year & n>340 | year == current.year) %>% mutate(idyr = paste0(pwsid,"-",year))
foo.cum <- demand.data %>% mutate(idyr = paste0(pwsid,"-",year)) %>% filter(idyr %in% foo.count$idyr) %>% arrange(pwsid, year, month, date2)
foo.cum <- foo.cum %>% distinct() %>% filter(year>=2000); #shorten for this file
foo.cum2 <- foo.cum %>% arrange(pwsid, year, julian) %>% dplyr::select(pwsid, year, date, julian, demand_mgd) %>% distinct() %>%
group_by(pwsid, year) %>% mutate(demand_mgd2 = ifelse(is.na(demand_mgd), 0, demand_mgd)) %>%  mutate(cum_demand = cumsum(demand_mgd2)) %>% dplyr::select(-demand_mgd, -demand_mgd2) %>% rename(demand_mgd = cum_demand) %>% distinct()
table(foo.cum$pwsid, foo.cum$year)
#in case duplicate days - take average
foo.cum3 <- foo.cum2 %>% group_by(pwsid, year, julian, date) %>% summarize(demand_mgd = round(mean(demand_mgd, na.rm=TRUE),2), .groups="drop") %>% distinct()
write.csv(foo.cum3, paste0(swd_data, "demand/all_demand_cum.csv"), row.names=FALSE)
######################################################################################################################################################################
#
# Reclaimed water data
#
#####################################################################################################################################################################
new_reclaimed <- subset(new_demand_by_mgd, select = -c(total,groundwater,boerne_lake,GBRA))
all_reclaimed <- rbind(old_reclaimed, new_reclaimed)
#include month abbreviations
all_reclaimed2 <- all_reclaimed %>% group_by(pwsid) %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
#calculate mean demand
all_reclaimed2 <- all_reclaimed2 %>% mutate(date = as.Date(substr(date,1,10),format='%Y-%m-%d'))
all_reclaimed3 <- all_reclaimed2 %>% group_by(pwsid) %>% arrange(date) %>% mutate(timeDays = as.numeric(date - lag(date)))
all_reclaimed4 <- all_reclaimed3 %>% group_by(pwsid) %>% mutate(mean_reclaimed = ifelse(timeDays <= 3, round(as.numeric(ma(reclaimed)),2), reclaimed),
julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
all_reclaimed5 <- all_reclaimed4 %>% mutate(reclaimed = round(reclaimed,2), mean_reclaimed = ifelse(is.na(mean_reclaimed)==TRUE, reclaimed, mean_reclaimed))
#calculate monthly peak
all_reclaimed6 <- all_reclaimed5 %>% group_by(pwsid, month, year) %>% mutate(peak_reclaimed = round(quantile(reclaimed, 0.98),1)); #took the 98% to omit outliers
#provide julian date
all_reclaimed7 <- all_reclaimed6 %>% mutate(date2 = date, date = paste0(monthAbb,"-",day(date2))) %>% select(-timeDays)
#write.csv
all_reclaimed8 <- subset(all_reclaimed7, select = c(pwsid, date, reclaimed, mean_reclaimed, julian, month, monthAbb, year, peak_reclaimed, date2))
write.csv(all_reclaimed8, paste0(swd_data, "demand/all_reclaimed_water.csv"), row.names=FALSE)
#calculate percent of total
all_reclaimed9 <- all_reclaimed8
all_reclaimed9$total <- all_demand_by_mgd$total
all_reclaimed9$percent_of_total <- (all_reclaimed9$reclaimed/all_reclaimed9$total)*100
#write.csv
write.csv(all_reclaimed9, paste0(swd_data, "demand/all_reclaimed_percent_of_total.csv"), row.names=FALSE)
######################################################################################################################################################################
#
# Read in new pop data
#
#####################################################################################################################################################################
all_city_data <- read_sheet("https://docs.google.com/spreadsheets/d/1BKb9Q6UFEBNsGrLZhjdq2kKX5t1GqPFCWF553afUKUg/edit#gid=2030520898", sheet = 1, range = "A4245:K", col_names = FALSE)
#filter for pop data only
all_pop_data <- all_city_data[,c("...1", "...10", "...11")]
#rename columns
pop_data <- rename(all_pop_data, date = "...1", clb_pop = "...10", wsb_pop = "...11")
pop_data <- as.data.frame(pop_data)
#remove na's
pop_data <- na.omit(pop_data)
#add julian indexing
nxx <- pop_data %>% mutate(year = year(date), day_month = substr(date, 6, 10))
for(i in 1:nrow(nxx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nxx$year[i]) == TRUE) {nxx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nxx$day_month[i]]}
if(leap_year(nxx$year[i]) == FALSE) {nxx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nxx$day_month[i]]}
print(paste(round(i/nrow(nxx)*100,2),"% complete"))
}
pop_data <- nxx
#split date by month and day
pop_data = pop_data %>%
mutate(date = ymd(date)) %>%
mutate_at(vars(date), funs(year, month, day))
#include pwsid
pop_data$pwsid <- "TX300001"
new_pop_data <- pop_data %>% filter(year >= 2022)
# merge old and new pop data
all_pop_data <- rbind(old_pop, new_pop_data)
#write.csv
write.csv(pop_data, paste0(swd_data, "demand/all_pop.csv"), row.names=FALSE)
################################################################################################################################################################
# remove all except for global environment
rm(list= ls()[!(ls() %in% c('julian.ref','update.date', 'current.month', 'current.year', 'end.date', 'end.year',
'mymonths', 'source_path', 'start.date', 'state_fips', 'stateAbb', 'stateFips', 'swd_data', 'today',
'%notin%', 'ma'))])
