table(check.last.date$date)
#write.csv
write.csv(demand_by_mgd, paste0(swd_data, "demand/all_demand_by_source.csv"), row.names=FALSE)
#include month abbreviations
demand2 <- all_demand_by_mgd %>% group_by(pwsid) %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
#calculate mean demand
demand2 <- all_demand_by_mgd %>% mutate(date = as.Date(substr(date,1,10),format='%Y-%m-%d'))
demand3 <- demand2 %>% group_by(pwsid) %>% arrange(date) %>% mutate(timeDays = as.numeric(date - lag(date)))
demand4 <- demand3 %>% group_by(pwsid) %>% mutate(mean_demand = ifelse(timeDays <= 3, round(as.numeric(ma(total)),2), total),
julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
demand5 <- demand4 %>% mutate(total = round(total,2), mean_demand = ifelse(is.na(mean_demand)==TRUE, total, mean_demand))
#calculate monthly peak
demand6 <- demand5 %>% group_by(pwsid, month, year) %>% mutate(peak_demand = round(quantile(total, 0.98),1)); #took the 98% to omit outliers
#provide julian date
demand7 <- demand6 %>% mutate(date2 = date, date = paste0(monthAbb,"-",day(date2))) %>% select(-timeDays)
#clean up
demand7 <- rename(demand7, demand_mgd = "total")
demand7 <- demand7[, c("pwsid", "date","demand_mgd", "mean_demand", "julian", "month", "monthAbb", "year", "peak_demand", "date2")]
#write.csv
write.csv(demand7, paste0(swd_data, "demand/all_total_demand.csv"), row.names=FALSE)
#create comulative demand
demand.data <- demand7 %>% filter(date2>start.date)
foo.count <- demand.data %>% group_by(pwsid, year) %>% count() %>% filter(year < current.year & n>340 | year == current.year) %>% mutate(idyr = paste0(pwsid,"-",year))
foo.cum <- demand.data %>% mutate(idyr = paste0(pwsid,"-",year)) %>% filter(idyr %in% foo.count$idyr) %>% arrange(pwsid, year, month, date2)
foo.cum <- foo.cum %>% distinct() %>% filter(year>=2000); #shorten for this file
foo.cum2 <- foo.cum %>% arrange(pwsid, year, julian) %>% dplyr::select(pwsid, year, date, julian, demand_mgd) %>% distinct() %>%
group_by(pwsid, year) %>% mutate(demand_mgd2 = ifelse(is.na(demand_mgd), 0, demand_mgd)) %>%  mutate(cum_demand = cumsum(demand_mgd2)) %>% dplyr::select(-demand_mgd, -demand_mgd2) %>% rename(demand_mgd = cum_demand) %>% distinct()
table(foo.cum$pwsid, foo.cum$year)
#in case duplicate days - take average
foo.cum3 <- foo.cum2 %>% group_by(pwsid, year, julian, date) %>% summarize(demand_mgd = round(mean(demand_mgd, na.rm=TRUE),2), .groups="drop") %>% distinct()
write.csv(foo.cum3, paste0(swd_data, "demand/all_demand_cum.csv"), row.names=FALSE)
######################################################################################################################################################################
#
# Reclaimed water data
#
#####################################################################################################################################################################
new_reclaimed <- subset(new_demand_by_mgd, select = -c(total,groundwater,boerne_lake,GBRA))
all_reclaimed <- rbind(old_reclaimed, new_reclaimed)
#include month abbreviations
all_reclaimed2 <- all_reclaimed %>% group_by(pwsid) %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
#calculate mean demand
all_reclaimed2 <- all_reclaimed2 %>% mutate(date = as.Date(substr(date,1,10),format='%Y-%m-%d'))
all_reclaimed3 <- all_reclaimed2 %>% group_by(pwsid) %>% arrange(date) %>% mutate(timeDays = as.numeric(date - lag(date)))
all_reclaimed4 <- all_reclaimed3 %>% group_by(pwsid) %>% mutate(mean_reclaimed = ifelse(timeDays <= 3, round(as.numeric(ma(reclaimed)),2), reclaimed),
julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month], year = year(date))
all_reclaimed5 <- all_reclaimed4 %>% mutate(reclaimed = round(reclaimed,2), mean_reclaimed = ifelse(is.na(mean_reclaimed)==TRUE, reclaimed, mean_reclaimed))
#calculate monthly peak
all_reclaimed6 <- all_reclaimed5 %>% group_by(pwsid, month, year) %>% mutate(peak_reclaimed = round(quantile(reclaimed, 0.98),1)); #took the 98% to omit outliers
#provide julian date
all_reclaimed7 <- all_reclaimed6 %>% mutate(date2 = date, date = paste0(monthAbb,"-",day(date2))) %>% select(-timeDays)
#write.csv
all_reclaimed8 <- subset(all_reclaimed7, select = c(pwsid, date, reclaimed, mean_reclaimed, julian, month, monthAbb, year, peak_reclaimed, date2))
write.csv(all_reclaimed8, paste0(swd_data, "demand/all_reclaimed_water.csv"), row.names=FALSE)
#calculate percent of total
all_reclaimed9 <- all_reclaimed8
all_reclaimed9$total <- all_demand_by_mgd$total
all_reclaimed9$percent_of_total <- (all_reclaimed9$reclaimed/all_reclaimed9$total)*100
#write.csv
write.csv(all_reclaimed9, paste0(swd_data, "demand/all_reclaimed_percent_of_total.csv"), row.names=FALSE)
######################################################################################################################################################################
#
# Read in new pop data
#
#####################################################################################################################################################################
all_city_data <- read_sheet("https://docs.google.com/spreadsheets/d/1BKb9Q6UFEBNsGrLZhjdq2kKX5t1GqPFCWF553afUKUg/edit#gid=2030520898", sheet = 1, range = "A4245:K", col_names = FALSE)
#filter for pop data only
all_pop_data <- all_city_data[,c("...1", "...10", "...11")]
#rename columns
pop_data <- rename(all_pop_data, date = "...1", clb_pop = "...10", wsb_pop = "...11")
pop_data <- as.data.frame(pop_data)
#remove na's
pop_data <- na.omit(pop_data)
#add julian indexing
nxx <- pop_data %>% mutate(year = year(date), day_month = substr(date, 6, 10))
for(i in 1:nrow(nxx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nxx$year[i]) == TRUE) {nxx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nxx$day_month[i]]}
if(leap_year(nxx$year[i]) == FALSE) {nxx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nxx$day_month[i]]}
print(paste(round(i/nrow(nxx)*100,2),"% complete"))
}
pop_data <- nxx
#split date by month and day
pop_data = pop_data %>%
mutate(date = ymd(date)) %>%
mutate_at(vars(date), funs(year, month, day))
#include pwsid
pop_data$pwsid <- "TX300001"
new_pop_data <- pop_data %>% filter(year >= 2022)
# merge old and new pop data
all_pop_data <- rbind(old_pop, new_pop_data)
#write.csv
write.csv(pop_data, paste0(swd_data, "demand/all_pop.csv"), row.names=FALSE)
################################################################################################################################################################
# remove all except for global environment
rm(list= ls()[!(ls() %in% c('julian.ref','update.date', 'current.month', 'current.year', 'end.date', 'end.year',
'mymonths', 'source_path', 'start.date', 'state_fips', 'stateAbb', 'stateFips', 'swd_data', 'today',
'%notin%', 'ma'))])
old.data.usace <- read.csv(paste0(swd_data, "reservoirs/usace_dams.csv"))
last.update <- max(old.data.usace$date); today <- as.Date(substr(Sys.time(),1,10), "%Y-%m-%d")
difftime(today, last.update, units = "weeks") #time diff check since last update
timeAmt = 2
timeUnit = "weeks"
mymonths <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec");
########################################################################################################################################
####### UPDATE ARMY CORPS ##############################################################################################################
########################################################################################################################################
######################################################################################################################################################################
#
#   UPDATE USACE WITH RECENT DATA: CANYON LAKE IS IN THIS DATASET
#
######################################################################################################################################################################
# #set up url
# baseURL = 'https://water.usace.army.mil/a2w/'
# report_url = "CWMS_CRREL.cwms_data_api.get_report_json?p_location_id="
# parameter_url <- paste0("&p_parameter_type=Stor%3AElev&p_last=", timeAmt, "&p_last_unit=", timeUnit, "&p_unit_system=EN&p_format=JSON");
#read in shapefile
project.df <- read_sf(paste0(swd_data, "reservoirs/usace_sites.geojson"))
#mapview::mapview(project.df)
ace.df <- project.df
#add url to data
res.url <- "http://water.usace.army.mil/a2w/f?p=100:1:0::::P1_LINK:"
project.df <- project.df %>% mutate(url_link = paste0(res.url,Loc_ID,"-CWMS"))
# Loop prep
#District list
tx.dist <- c("SWF", "SWT", "SWG", "ABQ")
data.dist <- unique(project.df$District)
tx.dist <- tx.dist[tx.dist %in% data.dist == TRUE] #No dams in ABQ
# API URL building blocks
baseURL = 'https://water.usace.army.mil/a2w/'
last_number = timeAmt; #number of units to collect
last_unit = timeUnit; #other options are months, weeks, and days
report_url = "CWMS_CRREL.cwms_data_api.get_report_json?p_location_id="
parameter_url <- paste0("&p_parameter_type=Stor%3AElev&p_last=", last_number, "&p_last_unit=", last_unit, "&p_unit_system=EN&p_format=JSON")
# Create final data frame
all_district_data <- as.data.frame(matrix(nrow=0, ncol=8)); colnames(all_district_data) <- c("date", "elev_Ft", "storage_AF", "fstorage_AF", "locid", "district", "NIDID", "name")
# Pull new data for all sites in all districts
for(j in 1:length(tx.dist)){ #loop through districts
district.id = tx.dist[j];
district_data <- as.data.frame(matrix(nrow=0, ncol=8)); colnames(district_data) <- c("date", "elev_Ft", "storage_AF", "fstorage_AF", "locid", "district", "NIDID", "name")
zt <- subset(project.df, District==district.id) %>% filter(str_detect(string = NIDID, pattern = "TX")) %>% filter(Loc_ID != 2165051) #Only TX, drop Truscott Brine Lake - no outward flow, no active mgmt - & different data structure breaks the loop
for (i in 1:length(zt$Loc_ID)){ #loop through sites within districts
location.id <- zt$Loc_ID[i]; location.id
full_url <- paste0(baseURL, report_url, location.id, parameter_url)
api.data <- GET(full_url, timeout(15000)) #use httr library to avoid timeout #CAN INCREASE IF TIMING OUT
dam.data <- jsonlite::fromJSON(content(api.data, 'text'), simplifyVector = TRUE, flatten=TRUE) ##
#lake level
lake_level <- dam.data$Elev[[1]]
lake_level <- lake_level %>% mutate(date = as.Date(substr(lake_level$time,1,11), "%d-%b-%Y")) %>% group_by(date) %>% summarize(elev_Ft = round(median(value, na.rm=TRUE),2), .groups = "drop") ##
plot(lake_level$date, lake_level$elev_Ft, type="l");
#lake storage
#conservation storage
cons.stor <- purrr::map(dam.data$`Conservation Storage`, ~ purrr::compact(.)) %>% purrr::keep(~length(.) != 0) #conservation storage = storage_AF
cons.stor <- cons.stor[[1]]
cons.stor <- cons.stor %>% mutate(date = as.Date(substr(cons.stor$time, 1, 11), "%d-%b-%Y")) %>% group_by(date) %>% summarize(storage_AF = round(median(value, na.rm=TRUE), 0), .groups ="drop")
#flood storage
flood.stor <- purrr::map(dam.data$`Flood Storage`, ~ purrr::compact(.)) %>% purrr::keep(~length(.) != 0)
flood.stor <- flood.stor[[1]]
flood.stor <- flood.stor %>% mutate(date = as.Date(substr(flood.stor$time, 1, 11), "%d-%b-%Y")) %>% group_by(date) %>% summarize(fstorage_AF = round(median(value, na.rm=TRUE), 0), .groups ="drop")
#combine storage
lake_stor <- merge(cons.stor, flood.stor, by.x = "date", by.y = "date", sort = TRUE)
plot(lake_stor$date, lake_stor$storage_AF, type="l"); lines(lake_stor$date, lake_stor$fstorage_AF, col="red") #igual
#combine lake data
lake_data <- merge(lake_level, lake_stor, by.x="date", by.y="date", all=TRUE)
lake_data$locid <- as.character(location.id);     lake_data$district <- district.id;
lake_data$NIDID <- as.character(zt$NIDID[i]);     lake_data$name <- as.character(zt$Name[i])
#bind to larger dataframe
district_data <- rbind(district_data, lake_data)
print(paste0(location.id,": ", as.character(zt$Name[i])))
} #end of site
all_district_data <- rbind(all_district_data, district_data) #save data from each district loop to master df
rm(api.data, dam.data, lake_level, cons.stor, flood.stor, lake_stor, lake_data, district_data)
} #end of district
summary(all_district_data)  #a few NA's in storage_AF and Elev_Ft - Addicks and Barker, as expected.
new.data.usace <- all_district_data
########################################################################################################################################################################################################################
#
#     ADD OLD AND NEW DATA TOGETHER
#
########################################################################################################################################################################################################################
#pull out unique reservoirs
unique.nid <- unique(new.data.usace$NIDID); unique.nid
#new data
nx <- new.data.usace
dateFormat = nx$date[1]
if(substr(dateFormat,5,5) == "-") {dateFormatFinal = "%Y-%m-%d"}
if(substr(dateFormat,5,5) != "-") {dateFormatFinal = "%m/%d/%Y"}
dateFormatFinal
nx$date <- as.Date(as.character(nx$date), dateFormatFinal)
nx$Year <- year(nx$date)
nx$day_month <- substr(nx$date, 6, 10)
#set julian values
for(i in 1:nrow(nx)) { #computationally slow. There's almost certainly a faster way. But it works.
if(leap_year(nx$Year[i]) == TRUE) {nx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == nx$day_month[i]]}
if(leap_year(nx$Year[i]) == FALSE) {nx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == nx$day_month[i]]}
print(paste(round(i/nrow(nx)*100,2),"% complete"))
}
#clean data
nx <- nx %>% mutate(elev_Ft = ifelse(elev_Ft <= 0, NA, elev_Ft), storage_AF = ifelse(storage_AF <=0, NA, storage_AF), fstorage_AF = ifelse(fstorage_AF <=0, NA, fstorage_AF))
maxCap <- nx %>% group_by(NIDID) %>% summarize(maxCap = 1.2*quantile(elev_Ft, 0.90, na.rm=TRUE), maxStor = 1.2*quantile(storage_AF, 0.90, na.rm=TRUE), maxfStor = 1.2*quantile(fstorage_AF, 0.90, na.rm=TRUE), .groups="drop");
nx <- nx %>% left_join(maxCap, by="NIDID") %>% mutate(elev_Ft = ifelse(elev_Ft > maxCap, NA, elev_Ft), storage_AF = ifelse(storage_AF > maxStor, NA, storage_AF), fstorage_AF = ifelse(fstorage_AF > maxfStor, NA, fstorage_AF)) %>%
select(-maxCap, -maxStor, -maxfStor)
#include month abbreviations
nx2 <- nx %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month])
#old data
fx <- old.data.usace
dateFormat = fx$date[1]
if(substr(dateFormat,5,5) == "-") {dateFormatFinal = "%Y-%m-%d"}
if(substr(dateFormat,5,5) != "-") {dateFormatFinal = "%m/%d/%Y"}
fx$date <- as.Date(as.character(fx$date), dateFormatFinal)
fx$Year <- year(fx$date)
fx$day_month <- substr(fx$date, 6, 10)
# #set julian values - SHOULDN't BE NEEDED GOING FORWARD.
# for(i in 1:nrow(fx)) { #computationally slow. There's almost certainly a faster way. But it works.
#
#   if(leap_year(fx$Year[i]) == TRUE) {fx$julian[i] <- julian.ref$julian_index_leap[julian.ref$day_month_leap == fx$day_month[i]]}
#   if(leap_year(fx$Year[i]) == FALSE) {fx$julian[i] <- julian.ref$julian_index[julian.ref$day_month == fx$day_month[i]]}
#
#   print(paste(round(i/nrow(fx)*100,2),"% complete"))
# }
#include month abbreviations
fx2 <- fx %>% mutate(julian = as.numeric(strftime(date, format = "%j")), month = month(date), monthAbb = mymonths[month])
#what is the most recent date?
old.last.date <- fx2 %>% group_by(NIDID) %>% filter(date == max(date, na.rm=TRUE)) %>% select(NIDID, date) %>% distinct() %>% rename(lastDate = date)
#remove anything new after that date
nx2 <- nx2 %>% left_join(old.last.date, by="NIDID") %>% filter(date > lastDate)
fx.2020 <- fx2 %>% filter(Year>=2020) %>% select(NIDID, day_month, OT_Ft, OT_AF) %>% distinct(); #2020 has complete data
nx2 <- merge(nx2, fx.2020, by.x=c("NIDID","day_month"), by.y=c("NIDID","day_month"), all.x=TRUE)
nx2 <- nx2 %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2))
nx2 <- nx2 %>% select(NIDID, name, date, Year, day_month, julian, elev_Ft, storage_AF, OT_Ft, OT_AF, percentStorage, monthAbb, month); #colnames(nx2) <- colnames(fx2)
#combine
fx <- rbind(fx2, nx2)
#make sure no duplicates
fx <- fx %>% distinct()
#arrange by NIDID and date
fx <- fx %>% arrange(NIDID, date)
#SCOTT KERR HAS HAD A NEW SEDIMENT SURVEY
scott.ot = 36639
fx <- fx %>% mutate(OT_AF = ifelse(NIDID == "NC00300" & Year >=2017, 36639, OT_AF))
fx <- fx %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2)) %>% mutate(storage_AF = ifelse(percentStorage > 300, NA, storage_AF), percentStorage = ifelse(percentStorage > 300, NA, percentStorage))
summary(fx)
tx.dams <- fx
tx.dams <- tx.dams %>% mutate(jurisdiction = "USACE")
write.csv(tx.dams, paste0(swd_data, "reservoirs/all_usace_dams.csv"), row.names=FALSE)
# filter out reservoir(s) of interest
canyon.lake <- tx.dams %>% filter(name == "Canyon Lake")
#usace changed reporting units so multiply by 1000 for those dates that are not in the same units as previous observations
canyon.lake <- canyon.lake %>% mutate(storage_AF = ifelse(storage_AF < 1000, storage_AF*1000, storage_AF))
#recalculate storage
canyon.lake <- canyon.lake %>% mutate(percentStorage = round(storage_AF/OT_AF*100,2))
write.csv(canyon.lake, paste0(swd_data, "reservoirs/all_reservoir_data.csv"), row.names=FALSE)
########################################################################################################################################################################################################################
#
#          UPDATE RESERVOIR STATUS AND STATS
#
########################################################################################################################################################################################################################
#fx <- read.csv(paste0(swd_data, "reservoirs/all_canyon_lake"), header = TRUE) #for picking up part-way
fx <- canyon.lake %>% filter(is.na(OT_Ft) == FALSE) #drop sites without operational target
unique.sites <- unique(fx$NIDID)
#set up data frame for stats and include year
stats <- as.data.frame(matrix(nrow=0,ncol=13));        colnames(stats) <- c("nidid", "julian", "min", "flow10", "flow25", "flow50", "flow75", "flow90", "max", "Nobs","startYr","endYr","date");
year.flow  <- as.data.frame(matrix(nrow=0, ncol=10));   colnames(year.flow) <- c("nidid", "name", "date", "year", "julian", "elev_ft","storage_af", "target_ft", "target_af", "percent_storage")
#for (i in 1:length(unique.sites)){  ##### loop not needed because it is only one site
#for (i in 17:length(unique.sites)){  #test
zt <- fx %>% filter(NIDID == unique.sites) %>% filter(Year >= year(start.date))
#summarize annual
zt.stats <- fx %>% group_by(NIDID, julian) %>% summarize(Nobs = n(), min=round(min(percentStorage, na.rm=TRUE),4), flow10 = round(quantile(percentStorage, 0.10, na.rm=TRUE),4), flow25 = round(quantile(percentStorage, 0.25, na.rm=TRUE),4),
flow50 = round(quantile(percentStorage, 0.5, na.rm=TRUE),4), flow75 = round(quantile(percentStorage, 0.75, na.rm=TRUE),4), flow90 = round(quantile(percentStorage, 0.90, na.rm=TRUE),4),
max = round(max(percentStorage, na.rm=TRUE),4), .groups="drop")
zt.stats <- zt.stats %>% mutate(NIDID = as.character(NIDID), startYr = min(fx$Year), endYr = max(fx$Year))
if(dim(zt.stats)[1] == 366) {zt.stats$date = julian.ref$day_month_leap}
if(dim(zt.stats)[1] == 365) {zt.stats$date = julian.ref$day_month[c(1:365)]}
#fill dataframe
stats <- rbind(stats, zt.stats)
zt <- fx %>% filter(Year>=2017) #%>% select(NIDID, name, date, year, julian, elev_Ft, storage_AF, OT_Ft, OT_AF, percentStorage, jurisdiction);
colnames(zt) <- c("NIDID", "name", "date", "year", "day_month", "julian", "elev_ft","storage_af", "target_ft", "target_af", "percent_storage", "month", "monthAbb", "jurisdiction")
year.flow <- rbind(year.flow, zt)
#    print(i)
#  }
bk.up <- stats
summary(stats)
summary(year.flow)
#fix date stuff
stats2 <- stats
stats2$endYr <- as.character(stats$endYr)
stats2$startYr <- as.character(stats$startYr)
stats2 <- stats2 %>% mutate(date2 = as.Date(paste0(end.year, "-",date)))
stats2 <- stats2 %>% mutate(month = substr(date,0,2))
stats2 <- stats2 %>% mutate(month = ifelse(month == "01", "Jan", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "07", "Jul", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "02", "Feb", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "08", "Aug", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "03", "Mar", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "09", "Sep", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "04", "Apr", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "10", "Oct", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "05", "May", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "11", "Nov", month))
stats2 <- stats2 %>% mutate(month = ifelse(month == "06", "Jun", month)); stats2 <- stats2 %>% mutate(month = ifelse(month == "12", "Dec", month))
#Now attach most recent value to stream stats for the map
recent.flow <- year.flow %>% group_by(NIDID) %>% filter(is.na(storage_af) == FALSE) %>% filter(date == max(date)); #do we want to do most recent date or most recent date with data?
current.stat <- merge(recent.flow, stats2, by.x=c("NIDID","julian"), by.y=c("NIDID", "julian"), all.x=TRUE) #%>% rename(date = date.x)
#clean
current.stat <- current.stat %>% select(-date.x, -year, -date.y, -elev_ft, -storage_af, -target_af, -target_ft, -month.x, -jurisdiction, -month.y)
current.stat <- current.stat %>% rename(date = day_month, month = monthAbb)
#if else for this year and last years flow
current.stat <- current.stat %>% mutate(status = ifelse(percent_storage <= flow10, "Extremely Dry", ifelse(percent_storage > flow10 & percent_storage <= flow25, "Very Dry", ifelse(percent_storage >= flow25 & percent_storage < flow50, "Moderately Dry",
ifelse(percent_storage >= flow50 & percent_storage < flow75, "Moderately Wet", ifelse(percent_storage >= flow75 & percent_storage < flow90, "Very Wet", ifelse(percent_storage >= flow90, "Extremely Wet", "Unknown")))))))
current.stat$status <- ifelse(is.na(current.stat$status), "unknown", current.stat$status)
table(current.stat$status)
#merge to sites geojson
project.df <- read_sf(paste0(swd_data, "reservoirs/all_canyon_lake_site.geojson"))
res.loc <- project.df %>% select(NIDID, Name, Jurisdiction, geometry) #why are there 4 of everything?
res.loc <- res.loc %>% slice(1)
canyon.lake.site.stats <- merge(res.loc, current.stat[,c("NIDID","status","percent_storage","julian","flow50")], by.x="NIDID", by.y="NIDID") #why are there 4 of everything?
#mapview::mapview(canyon.lake.site.stats)
geojson_write(canyon.lake.site.stats, file=paste0(swd_data, "reservoirs/all_canyon_lake_site.geojson"))
#rename nidid to site so can use same code as streamflow - used to make charts
current.year <- year.flow %>% filter(year == year(max(date)));     last.year <- year.flow %>% filter(year == (year(max(date))-1));
stats.flow <- merge(stats, current.year, by.x=c("NIDID","julian"), by.y=c("NIDID","julian"), all.x=TRUE) %>% rename(site = NIDID, flow = percent_storage)
stats.past <- merge(stats, last.year, by.x=c("NIDID", "julian"), by.y=c("NIDID", "julian"), all.x=TRUE) %>% rename(site = NIDID, flow = percent_storage) %>% as.data.frame()
#clean and bind
stats.flow <- stats.flow %>% select(-date.x, -name, -year, -elev_ft, -storage_af, -target_af, -target_ft, -month, -jurisdiction)
stats.flow <- stats.flow %>% rename(date = day_month, date2 = date.y, month = monthAbb)
stats.past <- stats.past %>% select(-date.x, -name, -year, -elev_ft, -storage_af, -target_af, -target_ft, -month, -jurisdiction)
stats.past <- stats.past %>% rename(date = day_month, date2 = date.y, month = monthAbb)
stats.flow <- rbind(stats.past, stats.flow)
#get status
stats.flow <- stats.flow %>% mutate(status = ifelse(flow <= flow10, "Extremely Dry", ifelse(flow > flow10 & flow <= flow25, "Very Dry", ifelse(flow >= flow25 & flow < flow50, "Moderately Dry",
ifelse(flow >= flow50 & flow < flow75, "Moderately Wet", ifelse(flow >= flow75 & flow < flow90, "Very Wet", ifelse(flow >= flow90, "Extremely Wet", "Unknown")))))))
stats.flow <- stats.flow %>% mutate(colorStatus = ifelse(status=="Extremely Dry", "darkred", ifelse(status=="Very Dry", "red", ifelse(status=="Moderately Dry", "orange", ifelse(status=="Moderately Wet", "cornflowerblue",
ifelse(status=="Very Wet", "blue", ifelse(status=="Extremely Wet", "navy", "gray")))))))
#save out
write.csv(stats.flow, paste0(swd_data, "reservoirs/all_reservoir_stats.csv"), row.names=FALSE)
################################################################################################################################################################
# remove all except for global environment
rm(list= ls()[!(ls() %in% c('julian.ref','update.date', 'current.month', 'current.year', 'end.date', 'end.year',
'mymonths', 'source_path', 'start.date', 'state_fips', 'stateAbb', 'stateFips', 'swd_data', 'today',
'%notin%', 'ma'))])
# Base URL & station ID list for API calls:
base.pcp.url <- "https://api.synopticdata.com/v2/stations/timeseries?stid=" #this is same for all sites
site.ids <- c("cict2", "twb03", "gubt2", "gbft2", "gbkt2", "gbjt2",
"gbrt2", "gbtt2", "gbvt2", "gbmt2", "gbst2", "gbdt2", "gbqt2",
"gupt2", "smct2", "ea004", "ea006", "ea035") #this is the part that changes
start_date = "202201010000" # this is the format needed for the listed website above
end_date <- today()-1 #allow a one day lag time
end_date <- as.character(end_date)
#end_date <- gsub('\\s+', '', end_date)
end_date <- gsub('-', '', end_date)
#end_date <- gsub(':', '', end_date)
#end_date <- substr(end_date, 1,12)
end_date <- gsub("^(.{8})(.*)$", "\\12345\\2", end_date)
url_time_start = paste0("&start=",start_date)
url_time_end = paste0("&end=",end_date)
addl.pars.url <- "&vars=precip_accum,precip_accum_since_local_midnight,precip_accum_one_hour,precip_accum_one_minute,precip_accum_five_minute,precip_accum_fifteen_minute&precip=1&units=english"
texmesonet.token <- "67d666866fcd4cd384aafea43b3184af"
url_token = paste0("&token=", texmesonet.token)
# Pull the data
# create empty storage dfs
synoptic.all.station.metadata <- matrix(nrow = 0, ncol = 15) %>% as.data.frame()
colnames(synoptic.all.station.metadata) <- c("STATUS", "MNET_ID", "ELEVATION", "NAME",
"STID", "ELEV_DEM", "LONGITUDE", "STATE",
"RESTRICTED", "QC_FLAGGED", "LATITUDE", "TIMEZONE", "ID",
"PERIOD_OF_RECORD.start", "PERIOD_OF_RECORD.end")
synoptic.all.station.data <- matrix(nrow = 0, ncol = 5) %>% as.data.frame()
colnames(synoptic.all.station.data) <- c("OBSERVATIONS.date_time", "OBSERVATIONS.precip_accumulated_set_1d",
"OBSERVATIONS.precip_intervals_set_1d", "station", "agency")
# create a list of assigned stations to their agencies
HADS <- c("CICT2", "GUBT2", "SMCT2")
TWDB <- c("TWB03")
EAA <- c("EA004", "EA006", "EA035")
GBRA <- c("GBFT2", "GBKT2", "GBJT2", "GBRT2", "GBTT2", "GBVT2", "GBMT2", "GBST2", "GBDT2", "GBQT2")
RAWS <- c("GUPT2")
# loop through sites and pull data
for(i in 1:length(site.ids)) {
api.url <- paste0(base.pcp.url, site.ids[i], url_time_start, url_time_end, addl.pars.url, url_token)
api.return <- fromJSON(api.url, flatten = TRUE)
api.station <- api.return$STATION
api.station.metadata <- subset(api.station, select=-c(16:24))
api.station.data <- subset(api.station, select=c(22:24))
api.station.data <- unnest(api.station.data, cols = c(OBSERVATIONS.date_time, OBSERVATIONS.precip_accumulated_set_1d,
OBSERVATIONS.precip_intervals_set_1d))
api.station.data$OBSERVATIONS.date_time <- as.Date(api.station.data$OBSERVATIONS.date_time)
#api.station.data <- do.call(rbind.data.frame, api.station.data)
api.station.data <- aggregate(.~OBSERVATIONS.date_time,data=api.station.data,FUN=sum)
api.station.data$station <- api.station.metadata[1,5]
api.station.data$agency <- case_when(
api.station.data$station %in% HADS ~ "HADS",
api.station.data$station %in% TWDB ~ "TWDB",
api.station.data$station %in% EAA ~ "EAA",
api.station.data$station %in% GBRA ~ "GBRA",
api.station.data$station %in% RAWS ~ "RAWS"
)
api.station.metadata$agency <- case_when(
api.station.metadata$STID %in% HADS ~ "HADS",
api.station.metadata$STID %in% TWDB ~ "TWDB",
api.station.metadata$STID %in% EAA ~ "EAA",
api.station.metadata$STID %in% GBRA ~ "GBRA",
api.station.metadata$STID %in% RAWS ~ "RAWS"
)
# Now bind it up to save out
synoptic.all.station.metadata <- rbind(synoptic.all.station.metadata, api.station.metadata)
synoptic.all.station.data <- rbind(synoptic.all.station.data, api.station.data)
# Keep an eye on the progress:
print(paste0("Completed pull for ", site.ids[i], ". ", round(i*100/length(site.ids), 2), "% complete."))
}
###################################################################################################################################################################################################################################
#
# Read in old pcp data
#
########################################################################################################################################################
boerne.sites <- read.csv(paste0(swd_data, "pcp/pcp_locations_metadata.csv"))
old.pcp <- read.csv(paste0(swd_data, "pcp/historic_pcp_data.csv")) %>% mutate(date = as.POSIXct(date, format = "%Y-%m-%d")) #historic data
current.year <-year(today);
# loop through sites and pull data
for(i in 1:length(site.ids)) {
api.url <- paste0(base.pcp.url, site.ids[i], url_time_start, url_time_end, addl.pars.url, url_token)
api.return <- fromJSON(api.url, flatten = TRUE)
api.station <- api.return$STATION
api.station.metadata <- subset(api.station, select=-c(16:24))
api.station.data <- subset(api.station, select=c(22:24))
api.station.data <- unnest(api.station.data, cols = c(OBSERVATIONS.date_time, OBSERVATIONS.precip_accumulated_set_1d,
OBSERVATIONS.precip_intervals_set_1d))
api.station.data$OBSERVATIONS.date_time <- as.Date(api.station.data$OBSERVATIONS.date_time)
#api.station.data <- do.call(rbind.data.frame, api.station.data)
api.station.data <- aggregate(.~OBSERVATIONS.date_time,data=api.station.data,FUN=sum)
api.station.data$station <- api.station.metadata[1,5]
api.station.data$agency <- case_when(
api.station.data$station %in% HADS ~ "HADS",
api.station.data$station %in% TWDB ~ "TWDB",
api.station.data$station %in% EAA ~ "EAA",
api.station.data$station %in% GBRA ~ "GBRA",
api.station.data$station %in% RAWS ~ "RAWS"
)
api.station.metadata$agency <- case_when(
api.station.metadata$STID %in% HADS ~ "HADS",
api.station.metadata$STID %in% TWDB ~ "TWDB",
api.station.metadata$STID %in% EAA ~ "EAA",
api.station.metadata$STID %in% GBRA ~ "GBRA",
api.station.metadata$STID %in% RAWS ~ "RAWS"
)
# Now bind it up to save out
synoptic.all.station.metadata <- rbind(synoptic.all.station.metadata, api.station.metadata)
synoptic.all.station.data <- rbind(synoptic.all.station.data, api.station.data)
# Keep an eye on the progress:
print(paste0("Completed pull for ", site.ids[i], ". ", round(i*100/length(site.ids), 2), "% complete."))
}
api.url <- paste0(base.pcp.url, cict2, url_time_start, url_time_end, addl.pars.url, url_token)
api.url <- paste0(base.pcp.url, "cict2", url_time_start, url_time_end, addl.pars.url, url_token)
api.return <- fromJSON(api.url, flatten = TRUE)
api.station <- api.return$STATION
api.station.metadata <- subset(api.station, select=-c(16:24))
packages = c("rstudioapi", "readxl",
"sf", "rgdal", "spData", "raster", "leaflet", "rmapshaper","geojsonio",
"tidycensus", "jsonlite", "rvest", "purrr", "httr",
"tidyverse", "lubridate", "plotly", "stringr", "rnoaa", "nhdplusTools",
"googlesheets4", "magrittr", "dplyr", "ckanr")
## Now load or install&load all
package.check <- lapply(
packages,
FUN = function(x) {
if (!require(x, character.only = TRUE)) {
install.packages(x, dependencies = TRUE)
library(x, character.only = TRUE)
}
}
)
#usgs packages
install.packages("dataRetrieval", repos=c("http://owi.usgs.gov/R", getOption("repos")))
library(dataRetrieval);  library(EGRET); #usgs links
######################################################################################################################################################################
######################################################################################################################################################################
#
#   SET GLOBAL VARIABLES
#
######################################################################################################################################################################
options(scipen=999) #changes scientific notation to numeric
rm(list=ls()) #removes anything stored in memory
#set working directory
# if working on a Windows use this to set working directory...
#source_path = rstudioapi::getActiveDocumentContext()$path
#setwd(dirname(source_path))
#swd_data <- paste0("..\\data\\")
# if working on a Mac use this to set working directory...
source_path = rstudioapi::getActiveDocumentContext()$path
setwd(dirname(source_path))
swd_data <- paste0("../data/")
#state info
stateAbb <- "TX"
stateFips <- 48
#variables used to update data
today = substr(Sys.time(),1,10); today;
current.year <- year(today);
start.date = "1990-01-01"; #set for the start of the period that we want to assess
end.date = paste0(year(today), "-12-31")
end.year = year(Sys.time())
mymonths <- c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"); #used below to convert numbers to abbrev
#save out update date for dashboard
update.date <- paste0(mymonths[month(today)]," ", day(today), ", ", end.year) %>% as.data.frame()
colnames(update.date) <- "today_date"
write.csv(update.date, paste0(swd_data, "update_date.csv"), row.names=FALSE)
#calculate moving average function
ma <- function(x,n=7){stats::filter(x,rep(1/n,n), sides=1)}
#useful functions
`%notin%` = function(x,y) !(x %in% y); #function to get what is not in the list
#standardized reference for setting julian values (drops leap days)
jan1 <- as.Date("2021-01-01")
dec31 <- as.Date("2021-12-31")
julian_index <- c(seq(jan1:dec31), "NA")
all.dates <- seq(as.Date(jan1), as.Date(dec31), by = "days")
day_month <- c(substr(all.dates,6,10), "NA")
day_month_leap <- c(day_month[1:59], "02-29", day_month[60:365])
julian_index_leap <- (1:366)
julian <- as.data.frame(matrix(nrow=366))
julian$day_month <- day_month; julian$julian_index <- julian_index
julian$day_month_leap <- day_month_leap; julian$julian_index_leap <- julian_index_leap
julian.ref <- julian %>% select(!V1)
rm(jan1, dec31, julian_index, all.dates, day_month,day_month_leap, julian_index_leap, julian)
install.packages("dataRetrieval", repos = c("http://owi.usgs.gov/R", getOption("repos")))
